{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "qUqmRF71xg4D",
        "UoRdn-yGx_DC",
        "cmHTqIdexmH8",
        "JjgBSS_7wX-N",
        "TpA-0_BZx4Ub",
        "Rd4TKQcHynDv",
        "PV0uMfxzxs0l"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "145d1b5ab3314c249a7ccbca8c2129fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_297949e5056f4052997302f1e7cb8f8e",
              "IPY_MODEL_4ec5d0fe5d5341279629c0baf6ae1a4a",
              "IPY_MODEL_d9f5e19ef94046b1b24361102e6923c3"
            ],
            "layout": "IPY_MODEL_0f83d58e77794160b72d3f5c1912420e"
          }
        },
        "297949e5056f4052997302f1e7cb8f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af8a2df40d8f402a9a85166f35e249fe",
            "placeholder": "​",
            "style": "IPY_MODEL_de1413f9a5ce4a389fe71cbe309c1328",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4ec5d0fe5d5341279629c0baf6ae1a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba83cc7c96f945158802b41c5adf678a",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9728fad0c62742e792591496f8051bcf",
            "value": 8
          }
        },
        "d9f5e19ef94046b1b24361102e6923c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89adc3c488284b9f90370b4f0032fd01",
            "placeholder": "​",
            "style": "IPY_MODEL_81bd25928edb4653a40eb427b07f4223",
            "value": " 8/8 [00:12&lt;00:00,  1.39s/it]"
          }
        },
        "0f83d58e77794160b72d3f5c1912420e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8a2df40d8f402a9a85166f35e249fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1413f9a5ce4a389fe71cbe309c1328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba83cc7c96f945158802b41c5adf678a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9728fad0c62742e792591496f8051bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89adc3c488284b9f90370b4f0032fd01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81bd25928edb4653a40eb427b07f4223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0778e99363b845fdb3876d2053798da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a68f7e52018e4ff0bf06fd219fd746e1",
              "IPY_MODEL_90bdea59fdc3424193f828da7eaf3b79",
              "IPY_MODEL_cd08155fd4c149c8ba5f0257dfece77f"
            ],
            "layout": "IPY_MODEL_383c92248e474e90b3a89609b6201bc2"
          }
        },
        "a68f7e52018e4ff0bf06fd219fd746e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b43e45061c044c318b1d2811e8bf5ad7",
            "placeholder": "​",
            "style": "IPY_MODEL_3f29231bd0924a88aa5516c7ccf5b44c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "90bdea59fdc3424193f828da7eaf3b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9490b1961a0c4b54a50496c98db2964d",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc7fb62409a44ce5b4823f788145b4f6",
            "value": 8
          }
        },
        "cd08155fd4c149c8ba5f0257dfece77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b04bd94a43b496c8570cd6c7dc1cff9",
            "placeholder": "​",
            "style": "IPY_MODEL_46f36eb5012b4315b11b97c4999a4200",
            "value": " 8/8 [00:12&lt;00:00,  1.37s/it]"
          }
        },
        "383c92248e474e90b3a89609b6201bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b43e45061c044c318b1d2811e8bf5ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f29231bd0924a88aa5516c7ccf5b44c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9490b1961a0c4b54a50496c98db2964d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7fb62409a44ce5b4823f788145b4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b04bd94a43b496c8570cd6c7dc1cff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46f36eb5012b4315b11b97c4999a4200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INFO"
      ],
      "metadata": {
        "id": "_rSkwmW5qWPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To infer (generate), Colab free is enough even for float16 (both Tesla T4 & no GPU).\n",
        "\n",
        "To train (finetune), you must will need at least Colab Pro or above. Remember to check if your GPU is A100. GPU memory > 20 GB is required."
      ],
      "metadata": {
        "id": "NME5QibVqY-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "生成的话，即使是原版 float16 精度，免费版 Colab 也跑得起来，包括 T4 显卡和无显卡。\n",
        "\n",
        "要训练，至少要 Colab Pro 或 Pro+, 记得检查 GPU 是不是 A100，需要 显存 > 20 GB。"
      ],
      "metadata": {
        "id": "me3mRmnjt1fY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "qUqmRF71xg4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "XRViHJfokAHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae6fadd-ad2d-45e8-9f8c-8ae83efb5e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 16 15:45:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt update\n",
        "# !apt install -qq -y nvidia-cuda-toolkit"
      ],
      "metadata": {
        "id": "s8tIH3UJJbY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np56CwwEJ8cY",
        "outputId": "c714ba9f-0a81-4355-f212-341e2ceec378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 安装依赖"
      ],
      "metadata": {
        "id": "Ep-GXSbvc4UR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG5LbTdJcr-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832b865b-f338-4556-cdc9-1454120f73c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Fujisaki' already exists and is not an empty directory.\n",
            "/content/Fujisaki\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ljsabc/Fujisaki\n",
        "%cd Fujisaki\n",
        "!sed -i 's/==/>=/g' requirements.txt\n",
        "%pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's recommended that you prepare your data locally. Colab Pro is so expensive..."
      ],
      "metadata": {
        "id": "j9hpSF6RvoWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp config_example.py config.py\n",
        "!sed -i 's/PARSE_REPLIES = False/PARSE_REPLIES = True/g' config.py\n",
        "import config\n",
        "config.PARSE_REPLIES = True"
      ],
      "metadata": {
        "id": "sRL6uWtiLR-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you prepare your data locally...\n",
        "# !cp -a /content/drive/MyDrive/twitter/tweets.md ."
      ],
      "metadata": {
        "id": "o-qfJMi0Kg3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "UoRdn-yGx_DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Fujisaki"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo57YU-LKvUl",
        "outputId": "f055a92b-0ee8-4e8f-aeb2-47b3a3f53c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Fujisaki\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./tokenize_dataset_rows.py --json_path ./tweets.md --save_path tweets.tokens --max_seq_length 240"
      ],
      "metadata": {
        "id": "q3cfC99cHusw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a994e3c-1707-450f-c9ec-5350b6359f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-8f82e7fc8ee2271c/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 7121.06it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1393.46it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-8f82e7fc8ee2271c/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 250.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python finetune.py --dataset_path tweets.tokens --lora_rank 16 --per_device_train_batch_size 8 --gradient_accumulation_steps 1 --num_train_epoch 3 --save_steps 2000 --save_total_limit 2 --learning_rate 2e-4 --fp16 --remove_unused_columns false --logging_steps 50 --output_dir output --warmup_steps 100"
      ],
      "metadata": {
        "id": "qGFAuly5H-Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Train in the notebook"
      ],
      "metadata": {
        "id": "OIGHenJju9Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer, HfArgumentParser\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training\n",
        "from dataclasses import dataclass, field\n",
        "import datasets\n",
        "import os"
      ],
      "metadata": {
        "id": "4G8oeRkfK3h3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5640987d-0fa9-4e34-c559-a16cb25f3879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('//172.28.0.1'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-2lb8h8qn39zvo --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from finetune import FinetuneArguments, CastOutputToFloat, data_collator, ModifiedTrainer"
      ],
      "metadata": {
        "id": "JDzghIGYPLOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, device_map='auto', revision=glm_rev)"
      ],
      "metadata": {
        "id": "ypTjXqFJzwjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('tweets.md', 'r', encoding='utf-8') as f:\n",
        "    tweets = json.load(f)\n",
        "\n",
        "tweets_count = len(tweets)\n",
        "del tweets\n",
        "tweets_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8QgkmTmy9vO",
        "outputId": "16b8e458-2e40-4fb1-e10a-55ab9bfc9b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9686"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_args, training_args = HfArgumentParser(\n",
        "    (FinetuneArguments, TrainingArguments)\n",
        ").parse_args_into_dataclasses(\n",
        "    ['--dataset_path', 'tweets.tokens',\n",
        "     '--lora_rank', '16',\n",
        "     '--per_device_train_batch_size', str(9700 // (40*45)),  # adjust yourself\n",
        "     '--gradient_accumulation_steps', '1',\n",
        "     '--num_train_epoch', '8',\n",
        "     '--save_steps', '2000',\n",
        "     '--save_total_limit', '2',\n",
        "     '--learning_rate', '2e-4',\n",
        "     '--remove_unused_columns', 'false',\n",
        "     '--logging_steps', '50',\n",
        "     '--output_dir', 'output',\n",
        "     '--warmup_steps', '100',\n",
        "     '--fp16'\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "CoDNHXXgK_B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_map = \"auto\"\n",
        "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "ddp = world_size != 1\n",
        "if ddp:\n",
        "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}"
      ],
      "metadata": {
        "id": "lPllMFm6K-_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(\n",
        "    \"THUDM/chatglm-6b\", trust_remote_code=True, device_map=device_map, revision=glm_rev\n",
        ")\n",
        "# , load_in_8bit=True"
      ],
      "metadata": {
        "id": "pTT1lCGbMQyD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "145d1b5ab3314c249a7ccbca8c2129fa",
            "297949e5056f4052997302f1e7cb8f8e",
            "4ec5d0fe5d5341279629c0baf6ae1a4a",
            "d9f5e19ef94046b1b24361102e6923c3",
            "0f83d58e77794160b72d3f5c1912420e",
            "af8a2df40d8f402a9a85166f35e249fe",
            "de1413f9a5ce4a389fe71cbe309c1328",
            "ba83cc7c96f945158802b41c5adf678a",
            "9728fad0c62742e792591496f8051bcf",
            "89adc3c488284b9f90370b4f0032fd01",
            "81bd25928edb4653a40eb427b07f4223"
          ]
        },
        "outputId": "aaba9368-01e6-4573-faba-79fd41eea4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "145d1b5ab3314c249a7ccbca8c2129fa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = prepare_model_for_int8_training(model)\n",
        "#model.gradient_checkpointing_enable()\n",
        "#model.enable_input_require_grads()\n",
        "model.is_parallelizable = True\n",
        "model.model_parallel = True\n",
        "#model.lm_head = CastOutputToFloat(model.lm_head)\n",
        "# model.config.use_cache = (\n",
        "#     False  # silence the warnings. Please re-enable for inference!\n",
        "# )"
      ],
      "metadata": {
        "id": "z4dbCfxAK-83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=finetune_args.lora_rank,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")"
      ],
      "metadata": {
        "id": "WH0P8aDXNBjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "A3dzU0scNTbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_from_disk(finetune_args.dataset_path)"
      ],
      "metadata": {
        "id": "RC13CkgdNTWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ModifiedTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "MG7bZaotNTTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "FOHuuQDjK-6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88d863a9-b089-4f27-bc0f-c3a5fe56833a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15504' max='15504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15504/15504 51:27, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>6.222600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.269800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.722900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.677800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.548700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>3.431100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.604700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.570700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.441600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.486400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.459100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>3.421500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>3.497000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>3.397300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.466900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>3.438000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>3.437400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>3.230300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.331700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>3.359500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>3.203700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>3.205900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.154200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>3.319400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>3.398500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>3.304200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>3.240400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>3.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.193200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>3.218100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>3.241200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>3.205500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>3.093000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>3.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.071600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>3.105400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>3.193700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>3.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.911000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>2.962400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>2.852300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>2.941200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>2.817800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>3.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>2.800800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>2.910500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>2.920600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>2.872900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.898900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>2.901400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>2.888600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>2.868700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>2.854300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>2.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>2.788700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>2.841200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>2.911400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>2.724200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.704400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>2.719000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>2.684200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>2.802700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>2.932700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>2.793000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>2.722800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>2.683500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>2.890900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>2.717600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>2.663900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>2.893400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>2.708400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>2.686300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>2.771400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>2.844100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>2.792300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>2.689900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>2.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>2.244100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.355400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>2.276900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>2.348500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>2.376200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>2.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>2.334700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>2.257300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>2.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>2.366000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>2.308500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>2.363100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4550</td>\n",
              "      <td>2.468400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>2.383600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>2.376500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>2.349100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>2.301100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>2.449800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4850</td>\n",
              "      <td>2.328800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>2.274100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>2.372500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.252900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5050</td>\n",
              "      <td>2.337600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>2.377500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5150</td>\n",
              "      <td>2.196300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>2.327200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>2.258600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>2.152100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5350</td>\n",
              "      <td>2.261800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>2.256900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5450</td>\n",
              "      <td>2.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>2.299700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5550</td>\n",
              "      <td>2.326500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>2.311400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5650</td>\n",
              "      <td>2.278000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>2.245900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>2.322500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>2.172400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5850</td>\n",
              "      <td>2.009500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>1.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5950</td>\n",
              "      <td>1.919100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.842100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6050</td>\n",
              "      <td>1.817800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>1.927700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6150</td>\n",
              "      <td>1.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>1.940500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6250</td>\n",
              "      <td>1.928800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>1.868800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6350</td>\n",
              "      <td>1.854600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>1.814000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6450</td>\n",
              "      <td>1.845800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.885500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6550</td>\n",
              "      <td>1.892400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>1.916700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6650</td>\n",
              "      <td>1.831900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>1.897600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6750</td>\n",
              "      <td>1.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>1.940500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6850</td>\n",
              "      <td>1.870800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>1.860700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6950</td>\n",
              "      <td>1.833100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7050</td>\n",
              "      <td>1.845400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>1.862700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7150</td>\n",
              "      <td>1.844300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>1.880000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7250</td>\n",
              "      <td>1.832400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>1.737300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7350</td>\n",
              "      <td>1.942700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>1.759100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7450</td>\n",
              "      <td>1.873900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>1.990300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7550</td>\n",
              "      <td>1.856000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>1.764800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7650</td>\n",
              "      <td>1.877300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>1.824600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7750</td>\n",
              "      <td>1.848200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>1.485400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7850</td>\n",
              "      <td>1.490700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>1.558400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7950</td>\n",
              "      <td>1.402000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.515100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8050</td>\n",
              "      <td>1.504900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>1.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8150</td>\n",
              "      <td>1.489700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>1.541400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8250</td>\n",
              "      <td>1.537000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>1.527700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8350</td>\n",
              "      <td>1.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>1.547300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8450</td>\n",
              "      <td>1.488700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>1.583200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8550</td>\n",
              "      <td>1.505900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>1.570200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8650</td>\n",
              "      <td>1.513400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>1.414000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8750</td>\n",
              "      <td>1.492600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>1.448700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8850</td>\n",
              "      <td>1.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>1.443800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8950</td>\n",
              "      <td>1.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.520500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9050</td>\n",
              "      <td>1.568500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>1.534000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9150</td>\n",
              "      <td>1.522900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>1.594900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9250</td>\n",
              "      <td>1.455300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>1.494900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9350</td>\n",
              "      <td>1.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>1.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9450</td>\n",
              "      <td>1.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>1.506000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9550</td>\n",
              "      <td>1.459700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>1.444600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9650</td>\n",
              "      <td>1.518100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>1.477300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9750</td>\n",
              "      <td>1.155100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>1.159600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9850</td>\n",
              "      <td>1.180100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>1.243900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9950</td>\n",
              "      <td>1.247800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.196300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10050</td>\n",
              "      <td>1.226300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>1.146400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10150</td>\n",
              "      <td>1.250800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>1.215600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10250</td>\n",
              "      <td>1.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>1.101200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10350</td>\n",
              "      <td>1.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>1.254800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10450</td>\n",
              "      <td>1.240100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>1.188700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10550</td>\n",
              "      <td>1.185800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10600</td>\n",
              "      <td>1.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10650</td>\n",
              "      <td>1.244400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10700</td>\n",
              "      <td>1.204400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10750</td>\n",
              "      <td>1.247800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>1.224000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10850</td>\n",
              "      <td>1.207200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10900</td>\n",
              "      <td>1.269800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10950</td>\n",
              "      <td>1.205300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.182400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11050</td>\n",
              "      <td>1.209900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>1.148900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11150</td>\n",
              "      <td>1.305100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>1.230900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11250</td>\n",
              "      <td>1.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11300</td>\n",
              "      <td>1.207100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11350</td>\n",
              "      <td>1.139600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>1.139800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11450</td>\n",
              "      <td>1.159300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>1.240000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11550</td>\n",
              "      <td>1.254800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>1.166000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11650</td>\n",
              "      <td>1.051700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>0.949200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11750</td>\n",
              "      <td>0.965800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11800</td>\n",
              "      <td>0.980600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11850</td>\n",
              "      <td>0.971500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11900</td>\n",
              "      <td>1.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11950</td>\n",
              "      <td>1.005500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.974600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12050</td>\n",
              "      <td>0.921100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12100</td>\n",
              "      <td>0.974900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12150</td>\n",
              "      <td>0.948300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12200</td>\n",
              "      <td>0.990600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12250</td>\n",
              "      <td>0.962600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12300</td>\n",
              "      <td>1.005800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12350</td>\n",
              "      <td>0.969200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>1.040800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12450</td>\n",
              "      <td>0.958800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.956300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12550</td>\n",
              "      <td>0.994500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12600</td>\n",
              "      <td>0.994300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12650</td>\n",
              "      <td>0.994900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12700</td>\n",
              "      <td>0.984900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12750</td>\n",
              "      <td>0.925900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>0.999200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12850</td>\n",
              "      <td>0.932800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12900</td>\n",
              "      <td>0.981100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12950</td>\n",
              "      <td>0.933300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>1.020700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13050</td>\n",
              "      <td>0.963300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13100</td>\n",
              "      <td>1.017400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13150</td>\n",
              "      <td>1.014100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>0.939500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13250</td>\n",
              "      <td>0.977100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13300</td>\n",
              "      <td>0.959200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13350</td>\n",
              "      <td>0.949200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13400</td>\n",
              "      <td>0.952500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13450</td>\n",
              "      <td>0.929400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.937900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13550</td>\n",
              "      <td>0.914000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13600</td>\n",
              "      <td>0.863500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13650</td>\n",
              "      <td>0.860300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13700</td>\n",
              "      <td>0.758900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13750</td>\n",
              "      <td>0.755400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13800</td>\n",
              "      <td>0.867300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13850</td>\n",
              "      <td>0.797100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13900</td>\n",
              "      <td>0.762700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13950</td>\n",
              "      <td>0.818700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.884200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14050</td>\n",
              "      <td>0.817600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14100</td>\n",
              "      <td>0.795000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14150</td>\n",
              "      <td>0.766100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14200</td>\n",
              "      <td>0.831000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14250</td>\n",
              "      <td>0.849300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14300</td>\n",
              "      <td>0.835100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14350</td>\n",
              "      <td>0.788500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14400</td>\n",
              "      <td>0.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14450</td>\n",
              "      <td>0.832400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.817800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14550</td>\n",
              "      <td>0.788300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14600</td>\n",
              "      <td>0.779800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14650</td>\n",
              "      <td>0.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14700</td>\n",
              "      <td>0.781800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14750</td>\n",
              "      <td>0.789800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14800</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14850</td>\n",
              "      <td>0.877800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14900</td>\n",
              "      <td>0.797700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14950</td>\n",
              "      <td>0.811200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.756000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15050</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15100</td>\n",
              "      <td>0.797300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15150</td>\n",
              "      <td>0.805000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15200</td>\n",
              "      <td>0.861800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15250</td>\n",
              "      <td>0.783600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15300</td>\n",
              "      <td>0.819700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15350</td>\n",
              "      <td>0.823500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15400</td>\n",
              "      <td>0.824000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15450</td>\n",
              "      <td>0.794600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.800100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15504, training_loss=1.8674790621019124, metrics={'train_runtime': 3089.0213, 'train_samples_per_second': 25.085, 'train_steps_per_second': 5.019, 'total_flos': 2.425995315478364e+17, 'train_loss': 1.8674790621019124, 'epoch': 8.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "uXj0UbHYK-3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('output')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikhUgr7AAcD6",
        "outputId": "33dc9f8d-803a-4649-883c-8b310df8b847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adapter_model.bin',\n",
              " 'runs',\n",
              " 'adapter_config.json',\n",
              " 'checkpoint-14000',\n",
              " 'checkpoint-12000']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import rmtree\n",
        "from distutils.dir_util import copy_tree"
      ],
      "metadata": {
        "id": "XIcAS0OqH99T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_tree(\"output\", \"/content/drive/MyDrive/twitter/output\")"
      ],
      "metadata": {
        "id": "v_P-Ro3CaWhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07103acf-9ce3-482d-f8bf-e46086cec540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/twitter/output/adapter_model.bin',\n",
              " '/content/drive/MyDrive/twitter/output/runs/Apr16_16-02-29_13a55f4ebd4e/1681661005.7692783/events.out.tfevents.1681661005.13a55f4ebd4e.5711.1',\n",
              " '/content/drive/MyDrive/twitter/output/runs/Apr16_16-02-29_13a55f4ebd4e/events.out.tfevents.1681661005.13a55f4ebd4e.5711.0',\n",
              " '/content/drive/MyDrive/twitter/output/runs/Apr16_16-06-46_13a55f4ebd4e/1681661245.260394/events.out.tfevents.1681661245.13a55f4ebd4e.7612.1',\n",
              " '/content/drive/MyDrive/twitter/output/runs/Apr16_16-06-46_13a55f4ebd4e/events.out.tfevents.1681661245.13a55f4ebd4e.7612.0',\n",
              " '/content/drive/MyDrive/twitter/output/adapter_config.json',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-14000/adapter_model.bin',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-14000/rng_state.pth',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-14000/scheduler.pt',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-14000/training_args.bin',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-14000/scaler.pt',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-14000/optimizer.pt',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-14000/trainer_state.json',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-12000/adapter_model.bin',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-12000/rng_state.pth',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-12000/scheduler.pt',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-12000/training_args.bin',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-12000/scaler.pt',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-12000/optimizer.pt',\n",
              " '/content/drive/MyDrive/twitter/output/checkpoint-12000/trainer_state.json']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QS6YdlFYz6oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "cmHTqIdexmH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gradio"
      ],
      "metadata": {
        "id": "97CBPD1laWcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "def fix_pytorch_int8():\n",
        "    valid_path = [p for p in sys.path if p and os.path.isdir(p)]\n",
        "\n",
        "    for path in valid_path:\n",
        "        for folder in os.listdir(path):\n",
        "            if 'torch' in folder:\n",
        "                packages_path = path\n",
        "                break\n",
        "\n",
        "    fix_path = f'{packages_path}/torch/nn/parameter.py'\n",
        "\n",
        "    with open(fix_path, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    if 'if data.dtype == torch.int8' not in text:\n",
        "        text = text.replace(\n",
        "            '            return torch.Tensor._make_subclass(cls, data, requires_grad)', \n",
        "            '            if data.dtype == torch.int8:\\n' \\\n",
        "            '                requires_grad = False\\n' \\\n",
        "            '            return torch.Tensor._make_subclass(cls, data, requires_grad)'\n",
        "        )\n",
        "        with open(fix_path, 'w') as f:\n",
        "            f.write(text)\n",
        "\n",
        "        return print('Fixed torch/nn/parameter.py')"
      ],
      "metadata": {
        "id": "ZaRHpWMAwTco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fix_pytorch_int8()"
      ],
      "metadata": {
        "id": "SzflAR27wUQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Fujisaki"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpY59K8sbbUm",
        "outputId": "558ec409-ea32-4c10-cf55-05d9b5e9e9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Fujisaki\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "from transformers import AutoTokenizer, GenerationConfig, AutoModel"
      ],
      "metadata": {
        "id": "fKGs1t6uwJmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA"
      ],
      "metadata": {
        "id": "JjgBSS_7wX-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FP16"
      ],
      "metadata": {
        "id": "kfw-Wng8wbi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FP16\n",
        "\n",
        "# torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"THUDM/chatglm-6b\",\n",
        "    trust_remote_code=True,\n",
        "    device_map='auto',\n",
        "    revision='4de8efe'\n",
        ").half().cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision='4de8efe')\n",
        "\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "peft_path = 'KumaTea/twitter'  # change it to your own\n",
        "model = PeftModel.from_pretrained(\n",
        "       model,\n",
        "       peft_path,\n",
        "       torch_dtype=torch.float16\n",
        "    )"
      ],
      "metadata": {
        "id": "DRnutF1NeiEh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0778e99363b845fdb3876d2053798da8",
            "a68f7e52018e4ff0bf06fd219fd746e1",
            "90bdea59fdc3424193f828da7eaf3b79",
            "cd08155fd4c149c8ba5f0257dfece77f",
            "383c92248e474e90b3a89609b6201bc2",
            "b43e45061c044c318b1d2811e8bf5ad7",
            "3f29231bd0924a88aa5516c7ccf5b44c",
            "9490b1961a0c4b54a50496c98db2964d",
            "cc7fb62409a44ce5b4823f788145b4f6",
            "7b04bd94a43b496c8570cd6c7dc1cff9",
            "46f36eb5012b4315b11b97c4999a4200"
          ]
        },
        "outputId": "e87fb52f-d24f-4405-84aa-0d561e2a4f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0778e99363b845fdb3876d2053798da8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INT8"
      ],
      "metadata": {
        "id": "msrfUTp0wyTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### INT8\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"KumaTea/twitter-int8\",  # change it to your own\n",
        "    trust_remote_code=True,\n",
        "    device_map='auto',\n",
        "    revision=\"1136001\"\n",
        ").half().cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision='4de8efe')"
      ],
      "metadata": {
        "id": "ggFUQpf2en_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INT4"
      ],
      "metadata": {
        "id": "bJzjYfdJxBnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### INT4\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"KumaTea/twitter-int4\",  # change it to your own\n",
        "    trust_remote_code=True,\n",
        "    device_map='auto',\n",
        "    revision=\"e2aecb2\"\n",
        ").half().cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision='4de8efe')"
      ],
      "metadata": {
        "id": "197BFzGoxDnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NO GPU"
      ],
      "metadata": {
        "id": "TpA-0_BZx4Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main difference:\n",
        "\n",
        "Remove `device_map='auto'`"
      ],
      "metadata": {
        "id": "8iuz_w4lyCzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FP16"
      ],
      "metadata": {
        "id": "USD7QOSVx9ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FP16\n",
        "\n",
        "# torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"THUDM/chatglm-6b\",\n",
        "    trust_remote_code=True,\n",
        "    revision='4de8efe'\n",
        ").float()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision='4de8efe')\n",
        "\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "peft_path = 'KumaTea/twitter'  # change it to your own\n",
        "model = PeftModel.from_pretrained(\n",
        "       model,\n",
        "       peft_path,\n",
        "       torch_dtype=torch.float16\n",
        "    )"
      ],
      "metadata": {
        "id": "Ugm96asDyBk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INT8"
      ],
      "metadata": {
        "id": "b3tM_KORyQyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### INT8\n",
        "\n",
        "# torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"KumaTea/twitter-int8\",  # change it to your own\n",
        "    trust_remote_code=True,\n",
        "    revision=\"1136001\"\n",
        ").float()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision='4de8efe')"
      ],
      "metadata": {
        "id": "tbQ50qR-ySgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INT4"
      ],
      "metadata": {
        "id": "KXw-RnEoyS2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### INT4\n",
        "\n",
        "# torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"int4\",\n",
        "    trust_remote_code=True,\n",
        "    revision=\"e2aecb2\"\n",
        ").float()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision='4de8efe')"
      ],
      "metadata": {
        "id": "T4tSOmmvyUc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer"
      ],
      "metadata": {
        "id": "Rd4TKQcHynDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "# torch.set_default_tensor_type(torch.FloatTensor)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "7Wv8zzKtgre2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(context, temperature, top_p, top_k):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        #repetition_penalty=1.1,\n",
        "        num_beams=1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        input_text = f\"Context: {context}Answer: \" \n",
        "        ids = tokenizer([input_text], return_tensors=\"pt\")\n",
        "        inputs = ids.to(\"cuda\")\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_length=224,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        out = out.tolist()[0]\n",
        "        decoder_output = tokenizer.decode(out)\n",
        "        out_text = decoder_output.split(\"Answer: \")[1]\n",
        "        return out_text\n",
        "\n",
        "\n",
        "def evaluate_stream(msg, history, temperature, top_p):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        #repetition_penalty=1.1,\n",
        "        num_beams=1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    history.append([msg, None])\n",
        "\n",
        "    context = \"\"\n",
        "    if len(history) > 4:\n",
        "        history.pop(0)\n",
        "\n",
        "    for j in range(len(history)):\n",
        "        history[j][0] = history[j][0].replace(\"<br>\", \"\")\n",
        "\n",
        "    # concatenate context\n",
        "    for h in history[:-1]:\n",
        "        context += h[0] + \"||\" + h[1] + \"||\"\n",
        "\n",
        "    context += history[-1][0]\n",
        "    context = context.replace(r'<br>', '')\n",
        "\n",
        "    # TODO: Avoid the tokens are too long.\n",
        "    CUTOFF = 224\n",
        "    while len(tokenizer.encode(context)) > CUTOFF:\n",
        "        # save 15 token size for the answer\n",
        "        context = context[15:]\n",
        "\n",
        "    h = []\n",
        "    print(\"History:\", history)\n",
        "    print(\"Context:\", context)\n",
        "    for response, h in model.stream_chat(tokenizer, context, h, max_length=CUTOFF, top_p=top_p, temperature=temperature):\n",
        "        history[-1][1] = response\n",
        "        yield history, \"\""
      ],
      "metadata": {
        "id": "J7PNWg3gFRJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [['你是谁','我是 kuma'],]\n",
        "r = []\n",
        "for h in evaluate_stream(\"你在干什么\", history, 1.0, 0.9):\n",
        "    r.append(h)\n",
        "print(r[-1][0][-1][-1])"
      ],
      "metadata": {
        "id": "egPtBitEHsz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c62132a-646b-4044-f792-773a3b648f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我在做实验，这是生物专业的(\n",
            "0.5824484310005573\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我在干什么不知道，我在玩游戏 🥺🥺🥺\n",
            "1.2787180160003118\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我并没有在工作或者学习 😗\n",
            "是玩游戏的 😗😗😗\n",
            "1.7947714659994745\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我昨天刚刚成年，所以是kuma的监护人\n",
            "0.6862346529997012\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我是kuma(\n",
            "我是猫娘！🥰\n",
            "而且我是大佬！🤤\n",
            "1.3786690090000775\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "在摸我的头 😋\n",
            "0.6267644339995968\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我本来打算今天回答你的一个提问的(\n",
            "0.7048833489998287\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "在玩游戏\n",
            "0.1911847259998467\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我准备起床了 🥺\n",
            "0.6388007280002057\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "什么都不知道，刚刚才看到你的回答，感觉被云了 😭\n",
            "1.2204573900007745\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我并没有在干什么\n",
            "我是 kuma 👀\n",
            "你在干什么\n",
            "1.027630020000288\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "哦哦 原来我是kuma的克星\n",
            "0.6418189589994654\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "在玩游戏 😗\n",
            "\n",
            "我kumakuma的图叫小苍 😗\n",
            "1.4690835499995956\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我正在输入这条消息的语音\n",
            "0.5050223759999426\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我没有做任何事情\n",
            "0.3108594039995296\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我？我昨天才被创建，昨天才注意到。\n",
            "0.8190949629997704\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "你在学日语 😗\n",
            "0.6199039679995622\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "我昨天刚刚注册了新账号，现在在打零工，赚钱买牛子，以后还会更多\n",
            "😋😋😋\n",
            "2.251341707999927\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "禁止不认识的人评价我(恼\n",
            "0.5590489709993562\n",
            "History: [['你是谁', '我是 kuma'], ['你在干什么', None]]\n",
            "Context: 你是谁||我是 kuma||你在干什么\n",
            "你在跟一个推油对话，好耶\n",
            "我被你忽视，很失落 🥺\n",
            "1.4371925839996038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive"
      ],
      "metadata": {
        "id": "PV0uMfxzxs0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "title = \"\"\"<h1 align=\"center\">李萌萌（Alter Ego）</h1>\n",
        "<h3 align=\"center\">这是一个通过ChatGLM模型训练的李萌萌的数字分身，你可以与她聊天，或者直接在文本框按下Enter，来观察李萌萌到底会说些什么。</h3>\"\"\"\n",
        "\n",
        "footer =  \"\"\"<p align='center'>项目在<a href='https://github.com/ljsabc/Fujisaki' target='_blank'>GitHub</a>上托管，基于清华的<a href='https://huggingface.co/THUDM/chatglm-6b' target='_blank'>THUDM/chatglm-6b</a>项目。</p>\n",
        "<p align='center'><em>\"I'm... a boy.\" --Chihiro Fujisaki</em></p>\"\"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.HTML(title)\n",
        "    state = gr.State()\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            temp = gr.components.Slider(minimum=0, maximum=1.1, value=0.95, label=\"Temperature\",\n",
        "                info=\"温度参数，越高的温度生成的内容越丰富，但是有可能出现语法问题。\")\n",
        "            top_p = gr.components.Slider(minimum=0.5, maximum=1.0, value=0.99, label=\"Top-p\",\n",
        "                info=\"top-p参数，只输出前p>top-p的文字，越大生成的内容越丰富，但也可能出现语法问题。数字越小似乎上下文的衔接性越好。\")\n",
        "            #code = gr.Textbox(label=\"temp_output\", info=\"解码器输出\")\n",
        "            #top_k = gr.components.Slider(minimum=1, maximum=200, step=1, value=25, label=\"Top k\",\n",
        "            #    info=\"top-k参数，下一个输出的文字会从top-k个文字中进行选择，越大生成的内容越丰富，但也可能出现语法问题。数字越小似乎上下文的衔接性越好。\")\n",
        "            \n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(label=\"聊天框\", info=\"\")\n",
        "            msg = gr.Textbox(label=\"输入框\", placeholder=\"最近过得怎么样？\",\n",
        "                info=\"输入你的内容，按[Enter]发送。也可以什么都不填写生成随机数据。\")\n",
        "            clear = gr.Button(\"清除聊天\")\n",
        "\n",
        "    msg.submit(evaluate_stream, [msg, chatbot, temp, top_p], [chatbot, msg])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "    gr.HTML(footer)\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(debug=False)\n"
      ],
      "metadata": {
        "id": "-Xy1tvCTApYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "gr.Interface(\n",
        "    fn=evaluate,\n",
        "    inputs=[\n",
        "        gr.components.Textbox(\n",
        "            lines=2, label=\"问题\", placeholder=\"最近过得怎么样？\",\n",
        "            info=\"可以在这里输入你的问题。也可以什么都不填写生成随机数据。\"\n",
        "        ),\n",
        "        #gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
        "        gr.components.Slider(minimum=0, maximum=1.1, value=1.0, label=\"Temperature\",\n",
        "            info=\"温度参数，越高的温度生成的内容越丰富，但是有可能出现语法问题。\"),\n",
        "        gr.components.Slider(minimum=0.5, maximum=1.0, value=0.99, label=\"Top p\",\n",
        "            info=\"top-p参数，只输出前p>top-p的文字，建议不要修改。\"),\n",
        "        gr.components.Slider(minimum=1, maximum=200, step=1, value=25, label=\"Top k\",\n",
        "            info=\"top-k参数，下一个输出的文字会从top-k个文字中进行选择，越大生成的内容越丰富，但也可能出现语法问题。数字越小似乎上下文的衔接性越好。\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.inputs.Textbox(\n",
        "            lines=5,\n",
        "            label=\"Output\",\n",
        "        )\n",
        "    ],\n",
        "    title=\"李萌萌（Alter Ego）\",\n",
        "    description=\"这是一个通过ChatGLM模型训练的李萌萌的数字分身，你可以在问题栏目填入内容，或者什么都不填，来观察李萌萌到底会说些什么。\",\n",
        ").launch()"
      ],
      "metadata": {
        "id": "GsS8ZYH5ficF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize"
      ],
      "metadata": {
        "id": "15XR1wIOxrbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Fujisaki"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtB3o3LYbnsY",
        "outputId": "d6a0f417-f46f-4c33-b75f-6a20121a42f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Fujisaki\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fix_pytorch_int8()"
      ],
      "metadata": {
        "id": "OW67l1BmQOw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "from transformers import AutoTokenizer, GenerationConfig, AutoModel"
      ],
      "metadata": {
        "id": "1Oauw_KGy4M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FP16\n",
        "\n",
        "# torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"THUDM/chatglm-6b\",\n",
        "    trust_remote_code=True,\n",
        "    device_map='auto',\n",
        "    revision='4de8efe'\n",
        ").cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision='4de8efe')\n",
        "\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "peft_path = 'KumaTea/twitter'      # change it to your own\n",
        "model = PeftModel.from_pretrained(\n",
        "       model,\n",
        "       peft_path,\n",
        "       torch_dtype=torch.float16\n",
        "    )"
      ],
      "metadata": {
        "id": "w5hmqJFvaRr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3449424a-a6ea-46e8-f83e-851409128265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('8013'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-2lb8h8qn39zvo --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Z5jChk_ObGiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7629eabb-3152-4d08-8582-8a5259fc0cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): ChatGLMForConditionalGeneration(\n",
              "      (transformer): ChatGLMModel(\n",
              "        (word_embeddings): Embedding(130528, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-27): 28 x GLMBlock(\n",
              "            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "            (attention): SelfAttention(\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "              (query_key_value): Linear(\n",
              "                in_features=4096, out_features=12288, bias=True\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
              "                )\n",
              "              )\n",
              "              (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            )\n",
              "            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GLU(\n",
              "              (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "              (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iM3hXiEQzi2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "id": "8kVxc0bybH8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e901807a-f748-4b3d-8442-8f77aaac7090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "peft.peft_model.PeftModelForCausalLM"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHAT'S AMAZING**\n",
        "\n",
        "↓↓↓"
      ],
      "metadata": {
        "id": "UE3089DBzkdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "jE8Pfnm0bXsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlLX5oGawRe5",
        "outputId": "2822338b-46db-4133-cd67-7500379be1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers_modules.THUDM.chatglm-6b.4de8efebc837788ffbfc0a15663de8553da362a2.modeling_chatglm.ChatGLMForConditionalGeneration"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XysXO8yfwVv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_int8 = model.quantize(8).half().cuda()"
      ],
      "metadata": {
        "id": "lubcHP5TwVt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_int8.save_pretrained('output/int8/')"
      ],
      "metadata": {
        "id": "0IXNg60SwZMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh output/int8/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9in16PAwlfE",
        "outputId": "e629e6d4-87d9-4d15-c1e6-ae558f1140aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 6.3G\n",
            "-rw-r--r-- 1 root root  870 Apr 16 17:16 config.json\n",
            "-rw-r--r-- 1 root root 4.2K Apr 16 17:16 configuration_chatglm.py\n",
            "-rw-r--r-- 1 root root  142 Apr 16 17:16 generation_config.json\n",
            "-rw-r--r-- 1 root root  57K Apr 16 17:16 modeling_chatglm.py\n",
            "-rw-r--r-- 1 root root 6.3G Apr 16 17:16 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root  15K Apr 16 17:16 quantization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_int8 is model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUU0d4FYEd3o",
        "outputId": "a37d6729-aee9-4b0a-bb05-e7b8ff504aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fp0X_Vllw5Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_int4 = model.quantize(4).half().cuda()"
      ],
      "metadata": {
        "id": "IAm7LhxywrIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('output/int4/')"
      ],
      "metadata": {
        "id": "bfRrbcw2w6H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh output/int4/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuj0eLUsw7Cu",
        "outputId": "d9ccbcce-c172-4647-9099-850ca8dd76a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3.7G\n",
            "-rw-r--r-- 1 root root  870 Apr 16 17:17 config.json\n",
            "-rw-r--r-- 1 root root 4.2K Apr 16 17:17 configuration_chatglm.py\n",
            "-rw-r--r-- 1 root root  142 Apr 16 17:17 generation_config.json\n",
            "-rw-r--r-- 1 root root  57K Apr 16 17:17 modeling_chatglm.py\n",
            "-rw-r--r-- 1 root root 3.7G Apr 16 17:17 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root  15K Apr 16 17:17 quantization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_int4 is model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OFVMzo9xNsy",
        "outputId": "ae1acbc0-4991-4f46-8279-344891bdfb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -av output/int8 /content/drive/MyDrive/twitter/quantized/\n",
        "!cp -av output/int4 /content/drive/MyDrive/twitter/quantized/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT052gs7xQdi",
        "outputId": "ca0ac56a-48a0-4a2e-fc17-50294ebad78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'output/int8/modeling_chatglm.py' -> '/content/drive/MyDrive/twitter/quantized/int8/modeling_chatglm.py'\n",
            "'output/int8/quantization.py' -> '/content/drive/MyDrive/twitter/quantized/int8/quantization.py'\n",
            "'output/int8/configuration_chatglm.py' -> '/content/drive/MyDrive/twitter/quantized/int8/configuration_chatglm.py'\n",
            "'output/int8/config.json' -> '/content/drive/MyDrive/twitter/quantized/int8/config.json'\n",
            "'output/int8/generation_config.json' -> '/content/drive/MyDrive/twitter/quantized/int8/generation_config.json'\n",
            "'output/int8/pytorch_model.bin' -> '/content/drive/MyDrive/twitter/quantized/int8/pytorch_model.bin'\n",
            "'output/int4/modeling_chatglm.py' -> '/content/drive/MyDrive/twitter/quantized/int4/modeling_chatglm.py'\n",
            "'output/int4/configuration_chatglm.py' -> '/content/drive/MyDrive/twitter/quantized/int4/configuration_chatglm.py'\n",
            "'output/int4/quantization.py' -> '/content/drive/MyDrive/twitter/quantized/int4/quantization.py'\n",
            "'output/int4/config.json' -> '/content/drive/MyDrive/twitter/quantized/int4/config.json'\n",
            "'output/int4/generation_config.json' -> '/content/drive/MyDrive/twitter/quantized/int4/generation_config.json'\n",
            "'output/int4/pytorch_model.bin' -> '/content/drive/MyDrive/twitter/quantized/int4/pytorch_model.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DtGs0aLixQbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}